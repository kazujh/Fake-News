{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14873286-6388-411a-bf9b-6422537739b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f0a0a7f-20d2-4ed7-8ff0-dff17a333994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc220ec1-0ef8-4c1f-aa26-9be56d1da4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6925d511-4681-4e6b-9124-60baefa0aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    if text[:4] == 'says':\n",
    "        text = text[4:]\n",
    "    elif text[:5] == 'print':\n",
    "        text = text[5:]\n",
    "    text = re.sub(r'-+|(\\n)+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffbc0b20-aede-4eaa-9546-5f458588b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['id', 'truth-value', 'statement', 'topics', 'speaker', 'speaker occupation', 'state', 'party', 'barely-true', 'false', 'half-true', 'true', 'POF', 'context']\n",
    "data = pd.DataFrame()\n",
    "current_data = pd.read_csv(\"./Liar dataset/train.tsv\", sep=\"\\t\", names=column_names)\n",
    "\n",
    "label_map = {\n",
    "    'POF': 0,\n",
    "    'false': 0,\n",
    "    'barely-true': 0,\n",
    "    'half-true': 1,\n",
    "    'true': 1\n",
    "}\n",
    "label_columns = ['POF', 'false', 'barely-true', 'half-true', 'true']\n",
    "current_data = current_data.dropna(subset=label_columns, how='all')\n",
    "current_data = current_data[['statement', 'POF', 'false', 'barely-true', 'half-true', 'true']]\n",
    "current_data['label'] = current_data[label_columns].idxmax(axis=1)\n",
    "current_data['truth'] = current_data['label'].map(label_map)\n",
    "\n",
    "\"\"\"weighted_sum = sum([current_data[col] * label_map[col] for col in label_columns])\n",
    "total_counts = current_data[label_columns].sum(axis=1)\n",
    "current_data['confidence'] = round((weighted_sum / total_counts), 2)\"\"\"\n",
    "current_data = current_data[['statement', 'truth']]\n",
    "current_data['statement'] = current_data['statement'].apply(preprocess)\n",
    "data = pd.concat([data, current_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb21cec9-6e34-41f0-9fa6-9c41f8338932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_data = pd.read_csv('./dataset 1/FakeNewsNet.csv')\n",
    "\n",
    "current_data = current_data[['title', 'real']]\n",
    "current_data = current_data.rename(columns={'title':'statement', 'real':'truth'})\n",
    "current_data['truth'] = current_data['truth'].astype(int)\n",
    "current_data['statement'] = current_data['statement'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a10f81fe-3507-4e25-ab3f-191901cbbd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6bc4eed-7adc-437c-ae00-4751d5b86767",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"X = current_data['statement']\n",
    "y = current_data['truth']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_vectorized, y)\n",
    "\n",
    "resampled_df = pd.DataFrame({'statement': vectorizer.inverse_transform(X_resampled),  # Attempt to reverse transform\n",
    "                             'truth': y_resampled})\n",
    "current_data = resampled_df\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cb8721d-9b77-416b-a1e7-041be99fecb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kandi burruss explodes over rape accusation on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>peoples choice awards  the best red carpet looks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sophia bush sends sweet birthday message to on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>colombian singer maluma sparks rumours of inap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gossip girl  years later how upper east siders...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23191</th>\n",
       "      <td>pippa middleton wedding in case you missed itp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23192</th>\n",
       "      <td>zayn malik  gigi hadids shocking split why the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23193</th>\n",
       "      <td>jessica chastain recalls the moment her mother...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23194</th>\n",
       "      <td>tristan thompson feels dumped after khloé kard...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23195</th>\n",
       "      <td>kelly clarkson performs a medley of kendrick l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23196 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               statement  truth\n",
       "0      kandi burruss explodes over rape accusation on...      1\n",
       "1       peoples choice awards  the best red carpet looks      1\n",
       "2      sophia bush sends sweet birthday message to on...      1\n",
       "3      colombian singer maluma sparks rumours of inap...      1\n",
       "4      gossip girl  years later how upper east siders...      1\n",
       "...                                                  ...    ...\n",
       "23191  pippa middleton wedding in case you missed itp...      1\n",
       "23192  zayn malik  gigi hadids shocking split why the...      0\n",
       "23193  jessica chastain recalls the moment her mother...      1\n",
       "23194  tristan thompson feels dumped after khloé kard...      0\n",
       "23195  kelly clarkson performs a medley of kendrick l...      1\n",
       "\n",
       "[23196 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "522e4e82-a3b2-4bb2-b7fb-545ac7fa170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, current_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d051dc16-0a4c-43f9-b022-558f3e8f0a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data_1 = pd.read_csv('./dataset 2/dataset/gossipcop_fake.csv')\n",
    "current_data_1 = current_data_1.dropna(how='all')\n",
    "current_data_1 = current_data_1[['title']]\n",
    "current_data_1 = current_data_1.rename(columns={'title':'statement'})\n",
    "current_data_1['truth'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "436e46dc-0598-48af-94ce-494583d44686",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data_2 = pd.read_csv('./dataset 2/dataset/gossipcop_real.csv')\n",
    "current_data_2 = current_data_2.dropna(how='all')\n",
    "current_data_2 = current_data_2[['title']]\n",
    "current_data_2 = current_data_2.rename(columns={'title':'statement'})\n",
    "current_data_2['truth'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "376a01dd-a081-41ce-af1e-e75401608cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"current_data = pd.concat([current_data_1, current_data_2])\n",
    "X = current_data['statement']\n",
    "y = current_data['truth']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_vectorized, y)\n",
    "\n",
    "resampled_df = pd.DataFrame({'statement': vectorizer.inverse_transform(X_resampled),  # Attempt to reverse transform\n",
    "                             'truth': y_resampled})\n",
    "current_data = resampled_df\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfea18fd-2697-41fa-89cb-b8db7578311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.concat([data, current_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a59ae85d-302c-470e-9675-fdcab90518df",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data_1 = pd.read_csv('./dataset 2/dataset/politifact_fake.csv')\n",
    "current_data_1 = current_data_1.dropna(how='all')\n",
    "current_data_1 = current_data_1[['title']]\n",
    "current_data_1 = current_data_1.rename(columns={'title':'statement'})\n",
    "current_data_1['truth'] = 0\n",
    "current_data_1['statement'] = current_data_1['statement'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0d45db9-1e16-45fe-a41e-0660969e2fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data_2 = pd.read_csv('./dataset 2/dataset/politifact_real.csv')\n",
    "current_data_2 = current_data_2.dropna(how='all')\n",
    "current_data_2 = current_data_2[['title']]\n",
    "current_data_2 = current_data_2.rename(columns={'title':'statement'})\n",
    "current_data_2['truth'] = 1\n",
    "current_data_2['statement'] = current_data_2['statement'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60cd9a58-2328-4b8b-9d7f-4870b358a53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"X = current_data['statement']\\ny = current_data['truth']\\n\\nvectorizer = TfidfVectorizer(max_features=5000)\\nX_vectorized = vectorizer.fit_transform(X)\\n\\nsmote = SMOTE(random_state=42)\\nX_resampled, y_resampled = smote.fit_resample(X_vectorized, y)\\n\\nresampled_df = pd.DataFrame({'statement': vectorizer.inverse_transform(X_resampled),  # Attempt to reverse transform\\n                             'truth': y_resampled})\\ncurrent_data = resampled_df\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_data = pd.concat([current_data_1, current_data_2])\n",
    "\"\"\"X = current_data['statement']\n",
    "y = current_data['truth']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_vectorized, y)\n",
    "\n",
    "resampled_df = pd.DataFrame({'statement': vectorizer.inverse_transform(X_resampled),  # Attempt to reverse transform\n",
    "                             'truth': y_resampled})\n",
    "current_data = resampled_df\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "366eb0da-c3f0-4659-a661-0dc80109f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, current_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3e8715d-15b4-4f4d-aa45-9be4af313a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data = pd.read_csv('./KaggleFakeNews/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36242d6b-6576-459a-a2d0-b4a3d9ed7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data = current_data.dropna(how='all')\n",
    "current_data = current_data.rename(columns={'text':'statement', 'label':'truth'})\n",
    "current_data = current_data[['statement', 'truth']]\n",
    "current_data['statement'] = current_data['statement'].astype(str).apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90f47a65-0c4d-4ffa-a86a-544f44ae1251",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, current_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23019fe9-2a80-4960-b95c-80722f338503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d8277c1-2200-4031-b7d4-96d6d587866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b98c81c0-d3a4-4cfa-9437-57e7571c14e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        ...\n",
    "        \n",
    "    def sigmoid(self, n):\n",
    "        return 1 / (1 + np.exp(-n))\n",
    "        \n",
    "    def initialize_weights(self, n_features):\n",
    "        weights = np.zeros(n_features)\n",
    "        bias = 0\n",
    "        return weights, bias\n",
    "        \n",
    "    def predict(self, X, weights, bias):\n",
    "        linear_model = X.dot(weights) + bias\n",
    "        predictions = self.sigmoid(linear_model)\n",
    "        return predictions\n",
    "        \n",
    "    def calculate_loss(self, y_true, y_pred):\n",
    "        n = len(y_true)\n",
    "        loss = (-1/n) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def gradient_descent(self, X, y, weights, bias, lr):\n",
    "        n = X.shape[0]\n",
    "    \n",
    "        y_pred = self.predict(X, weights, bias)\n",
    "    \n",
    "        dw = X.T.dot(y_pred - y) / n\n",
    "        db = np.sum(y_pred - y) / n\n",
    "    \n",
    "        weights -= lr * dw\n",
    "        bias -= lr * db\n",
    "    \n",
    "        return weights, bias\n",
    "\n",
    "    def train(self, X, y, lr=.1, epochs=1000, batch_size=500):\n",
    "        n_features = X.shape[1]\n",
    "    \n",
    "        weights, bias = self.initialize_weights(n_features)\n",
    "    \n",
    "        losses = []\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i:i + batch_size]\n",
    "                y_batch = y[i:i + batch_size]\n",
    "                \n",
    "                \n",
    "                weights, bias = self.gradient_descent(X_batch, y_batch, weights, bias, lr)\n",
    "    \n",
    "            y_pred = self.predict(X, weights, bias)\n",
    "            loss = self.calculate_loss(y, y_pred)\n",
    "            losses.append(loss)\n",
    "    \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "        return weights, bias, losses\n",
    "\n",
    "    def classify(self, X, weights, bias, threshold=.5):\n",
    "        probabilities = self.predict(X, weights, bias)\n",
    "        return [1 if p >= threshold else 0 for p in probabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5354f503-a2aa-408d-8349-cabd6a0bf291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE FOR THE SKLEARN MODEL\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "X = data['statement'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "89a4bd9f-4d6c-4346-9374-bf2c55864d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE FOR THE SKLEARN MODEL\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "y = np.array(data['truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8b3e855c-bd4c-4926-b812-61dd9b8f2ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=.2, random_state=42) #USE FOR THE SKLEARN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "721ad222-a6d1-4f50-8a0a-f212a8581cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DON'T USE FOR SKLEARN\n",
    "#regressor = LogisticRegression()\n",
    "#weights, bias, losses = regressor.train(X_train, y_train)\n",
    "#y_pred = regressor.classify(X_test, weights, bias)\n",
    "#print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "#print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c133e1b9-c4c2-487f-8aa4-deac4b638013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12fbf28d-6a33-40ca-adc8-c08946200649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7e2b0fe6-ecce-4dd0-b626-fb6f8d896be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "#model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4cc172d2-9f68-40c8-89b6-7cd79b849fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = model.predict(X_test)\n",
    "#y_prob = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8c2b0741-90a3-477b-88fe-9ea8942a9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "#print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2cd98408-7393-4365-8b96-8063e9be447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a8ff251-d28b-4b2c-8719-0a4cad914507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import issparse, csr_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb823d56-2151-406b-9a03-0b7494caa10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.grow_tree(X, y, 0)\n",
    "\n",
    "    def grow_tree(self, X, y, depth):\n",
    "\n",
    "        if depth >= self.max_depth or X.shape[0] < self.min_samples_split:\n",
    "            return np.bincount(y).argmax()\n",
    "\n",
    "        best_feature, best_threshold = self.find_best_split(X, y)\n",
    "        if best_feature is None:\n",
    "            return np.bincount(y).argmax()\n",
    "        X_col = X[:, best_feature].toarray().flatten()    \n",
    "        left_indices = X_col <= best_threshold\n",
    "        right_indices = X_col > best_threshold\n",
    "\n",
    "        if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
    "            # Return majority class if split is invalid\n",
    "            return np.bincount(y).argmax()\n",
    "        left = self.grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right = self.grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        return {'feature': best_feature, 'threshold': best_threshold, 'left': left, 'right': right}\n",
    "\n",
    "    def find_best_split(self, X, y):\n",
    "        #iterate through every split and test gini\n",
    "        n_features = X.shape[1]\n",
    "        features = np.random.choice(n_features, int(np.sqrt(n_features)), replace=False)\n",
    "        best_gini = 1.0\n",
    "        best_feature, best_threshold = None, None\n",
    "        for feature in features:\n",
    "            X_col = X[:, feature].toarray().flatten()\n",
    "            thresholds = np.unique(X_col[X_col > 0])\n",
    "            for threshold in thresholds:\n",
    "                left_indices = X_col <= threshold\n",
    "                right_indices = X_col > threshold\n",
    "                groups = [y[left_indices], y[right_indices]]\n",
    "\n",
    "                gini = gini_impurity(groups, np.unique(y))\n",
    "\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_threshold = threshold\n",
    "                    best_feature = feature\n",
    "            \n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _predict_tree(self, X):\n",
    "        predictions = []\n",
    "        for row in X:\n",
    "            node = self.tree\n",
    "            while isinstance(node, dict):\n",
    "                if row[node['feature']] <= node['threshold']:\n",
    "                    node = node['left']\n",
    "                else:\n",
    "                    node = node['right']\n",
    "            predictions.append(node)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2f15256f-e3f1-4f13-b726-c22aeeb781b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(groups, classes):\n",
    "    n_instances = sum([len(group) for group in groups])\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = len(group)\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        for class_val in classes:\n",
    "            proportion = list(group).count(class_val) / size\n",
    "            score += proportion ** 2\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "16723fe5-0734-4b25-96c7-dfa05f3c7b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=5, min_samples_split=2):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_trees):\n",
    "            X_sample, y_sample = random_sample(X, y)\n",
    "\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X, y)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_dense = X.toarray() if hasattr(X, \"toarray\") else X  # Handle sparse matrices\n",
    "        predictions = np.array([tree._predict_tree(X_dense) for tree in self.trees])\n",
    "        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d2336937-32f2-4674-a052-80812c62f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(X, y):\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c47ef93a-2586-4c49-96cd-e03b1a54ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8e1bbc6c-d9ad-4193-b47d-dff64abf3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c0f2e0ef-967b-47e5-a9bd-67db5f135366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b7aad927-de89-420f-bd8b-66a60d092a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "#print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96741faf-63a2-4339-8f3b-3e5e8b3083a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assume X parameter will be sparse\n",
    "#print(X_train[0].indices)\n",
    "#print(X_train[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d039ecad-0432-4f94-bdbd-f78ed8147186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf = RandomForest(n_trees=100, max_depth=10000)\n",
    "#rf.fit(X_train, y_train)\n",
    "\n",
    "#y_pred = rf.predict(X_test)\n",
    "\n",
    "#accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "#print(f'accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb0e8a43-e519-4ab3-b1b9-20e18363aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import MobileBertTokenizer, TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8084530b-fbc6-4e9a-b0f6-01b1f2bbebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b196fe5-75e3-4f89-b8f4-403f526ffea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert = data['statement'].astype(str)\n",
    "y_bert = np.array(data['truth'])\n",
    "X_bert_train, X_bert_test, y_bert_train, y_bert_test = train_test_split(X_bert, y_bert, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "300b450d-8f0d-45bb-88a6-50b3840c1d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2c78d66-3446-45b0-83f9-bb86a26ff8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert_train = X_bert_train.tolist()\n",
    "X_bert_test = X_bert_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30591cc1-1b0b-4ba1-bc61-aaa5415d3c9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(\n",
    "    X_bert_train,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=64,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "test_tokenized = tokenizer(\n",
    "    X_bert_test,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=64,\n",
    "    return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ff8edad-f5c4-43a4-8501-12469a532f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': train_tokenized['input_ids'], 'attention_mask': train_tokenized['attention_mask']},\n",
    "    y_bert_train\n",
    ")).shuffle(len(X_bert_train)).batch(16)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': test_tokenized['input_ids'], 'attention_mask': test_tokenized['attention_mask']},\n",
    "    y_bert_test\n",
    ")).batch(16)\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cd743963-1d86-46aa-80de-68292e33745c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./new_model were not used when initializing TFMobileBertForSequenceClassification: ['dropout_49']\n",
      "- This IS expected if you are initializing TFMobileBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMobileBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFMobileBertForSequenceClassification were initialized from the model checkpoint at ./new_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMobileBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained('./new_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6f6cdd05-36ae-4e80-be5d-eb7fb68b3fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564/564 [==============================] - 673s 1s/step - loss: nan - accuracy: 0.5013 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[nan, 0.501329779624939]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f98f8317-1073-48fb-8063-e5dd9fcb52f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMobileBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFMobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained('google/mobilebert-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "876287c7-f9b0-48bd-ac49-9aa9851826ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamWeightDecay\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamWeightDecay(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16e14d9a-73ef-41db-bc94-ecb1b636a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79f8fa12-41bf-4851-80b5-1a6582f85817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed precision policy set: <DTypePolicy \"mixed_float16\">\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.mixed_precision import Policy, set_global_policy\n",
    "\n",
    "# Set the global policy for mixed precision\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)\n",
    "\n",
    "print(\"Mixed precision policy set:\", policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33d2a8-2c5d-455d-adde-e6fcdb0ccb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From C:\\Users\\kazuj\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kazuj\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "   9/2765 [..............................] - ETA: 1:18:45 - loss: 134093.4219 - accuracy: 0.4375 "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=3, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c221d2-f293-4b7c-8abd-0174a9af31ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./new_model')\n",
    "tokenizer.save_pretrained('./new_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1cbc4a71-53a3-4e3e-99b2-dd9f966045f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"./Liar dataset/test.tsv\", sep=\"\\t\", names=column_names)\n",
    "\n",
    "label_map = {\n",
    "    'POF': 0,\n",
    "    'false': 0,\n",
    "    'barely-true': 0,\n",
    "    'half-true': 1,\n",
    "    'true': 1\n",
    "}\n",
    "label_columns = ['POF', 'false', 'barely-true', 'half-true', 'true']\n",
    "\n",
    "test_data = test_data.dropna(subset=label_columns, how='all')\n",
    "test_data = test_data[['statement', 'POF', 'false', 'barely-true', 'half-true', 'true']]\n",
    "test_data['label'] = test_data[label_columns].idxmax(axis=1)\n",
    "test_data['truth'] = test_data['label'].map(label_map)\n",
    "\n",
    "\"\"\"weighted_sum = sum([current_data[col] * label_map[col] for col in label_columns])\n",
    "total_counts = current_data[label_columns].sum(axis=1)\n",
    "current_data['confidence'] = round((weighted_sum / total_counts), 2)\"\"\"\n",
    "test_data = test_data[['statement', 'truth']]\n",
    "test_data['statement'] = test_data['statement'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aa728d16-a131-4aee-8da2-138fb106d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = test_data['statement'].astype(str).tolist()\n",
    "labels = test_data['truth']\n",
    "\n",
    "tst_tokenized = tokenizer(\n",
    "    feats,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=64,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "tst_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': tst_tokenized['input_ids'], 'attention_mask': tst_tokenized['attention_mask']},\n",
    "    labels\n",
    ")).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "33b29b73-7802-4b8b-8b1a-dac386b20186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kazuj\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\generation\\tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48ec6b-92c6-4450-95dc-75621f3db5e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = len(test_tokenized['input_ids']) // 32  # Adjust based on your hardware\n",
    "outputs = []\n",
    "\n",
    "for i in tqdm(range(0, len(test_tokenized['input_ids']), batch_size)):\n",
    "    batch_input_ids = test_tokenized['input_ids'][i:i+batch_size]\n",
    "    batch_attention_mask = test_tokenized['attention_mask'][i:i+batch_size]\n",
    "    batch_output = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "    outputs.append(batch_output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a64b643-b376-4898-acf5-cb0925bcedcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits = tf.concat(outputs, axis=0)\n",
    "\n",
    "print(all_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a88ffee-3026-428d-a5d4-62b914a8f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = tf.argmax(all_logits, axis=-1)\n",
    "correct_predictions = tf.reduce_sum(tf.cast(predicted_classes == y_bert_test, tf.float32))\n",
    "accuracy = correct_predictions / len(y_bert_test)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy.numpy():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "232f0674-98ff-4a7b-8ac3-d908757e6781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./new_model were not used when initializing TFMobileBertForSequenceClassification: ['dropout_49']\n",
      "- This IS expected if you are initializing TFMobileBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMobileBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFMobileBertForSequenceClassification were initialized from the model checkpoint at ./new_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMobileBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8403a18b-fdf7-4f63-910c-84dd4be64dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fcae8e-ae90-406c-bbdf-33902626fc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
