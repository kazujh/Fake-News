{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14873286-6388-411a-bf9b-6422537739b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f0a0a7f-20d2-4ed7-8ff0-dff17a333994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc220ec1-0ef8-4c1f-aa26-9be56d1da4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6925d511-4681-4e6b-9124-60baefa0aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    if text[:4] == 'says':\n",
    "        text = text[4:]\n",
    "    elif text[:5] == 'print':\n",
    "        text = text[5:]\n",
    "    text = re.sub(r'-+', ' ', text)\n",
    "    text = re.sub(r'_+', ' ', text)\n",
    "    text = re.sub(r'(\\n)+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34e1bb66-6dc0-462e-9a45-44268a7b9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dfs(df1, df2):\n",
    "    return pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10f81fe-3507-4e25-ab3f-191901cbbd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import wordnet\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca77dcc9-4bce-4c8f-a9f0-b2eb46520281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            add = random.choice(synonyms[0].lemmas())\n",
    "            new_words.append(add.name())\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffbc0b20-aede-4eaa-9546-5f458588b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['id', 'truth-value', 'statement', 'topics', 'speaker', 'speaker occupation', 'state', 'party', 'barely-true', 'false', 'half-true', 'true', 'POF', 'context']\n",
    "df = pd.DataFrame()\n",
    "current_df = pd.read_csv(\"./Liar dataset/train.tsv\", sep=\"\\t\", names=column_names)\n",
    "\n",
    "\n",
    "label_map = {\n",
    "    'POF': 0,\n",
    "    'false': 0,\n",
    "    'barely-true': 0,\n",
    "    'half-true': 1,\n",
    "    'true': 1\n",
    "}\n",
    "label_columns = ['POF', 'false', 'barely-true', 'half-true', 'true']\n",
    "current_df = current_df.dropna(subset=label_columns, how='all')\n",
    "current_df = current_df[['statement', 'POF', 'false', 'barely-true', 'half-true', 'true']]\n",
    "current_df['label'] = current_df[label_columns].idxmax(axis=1)\n",
    "current_df['truth'] = current_df['label'].map(label_map)\n",
    "\n",
    "\"\"\"weighted_sum = sum([current_data[col] * label_map[col] for col in label_columns])\n",
    "total_counts = current_data[label_columns].sum(axis=1)\n",
    "current_data['confidence'] = round((weighted_sum / total_counts), 2)\"\"\"\n",
    "current_df = current_df[['statement', 'truth']]\n",
    "current_df['statement'] = current_df['statement'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4cb3041-4f49-472f-a43a-ca28cc9f2be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_df = current_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74f2845a-7c8f-41a0-84ce-0e03be143dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrased_statements = pd.Series([paraphrase(statement) for statement in current_df[:len(current_df) // 2]['statement']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8525771e-359f-44c1-9084-b8399b0e7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrased_statements = paraphrased_statements.apply(preprocess)\n",
    "paraphrased_df = pd.DataFrame({'statement': paraphrased_statements, 'truth': current_df['truth'][:len(current_df) // 2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fa907f5-95a4-4758-b472-c29a3d6d64b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_df = combine_dfs(paraphrased_df, current_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21bb8f06-a8a9-4d50-871c-9dd7b496df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampler = RandomOverSampler(random_state=SEED)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(current_df[['statement']], current_df['truth'])\n",
    "oversampled_df = pd.DataFrame({'statement': X_resampled['statement'], 'truth': y_resampled})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6260cc58-8196-4838-9501-686ac20753bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combine_dfs(df, oversampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb21cec9-6e34-41f0-9fa6-9c41f8338932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_df = pd.read_csv('./dataset 1/FakeNewsNet.csv')\n",
    "\n",
    "current_df = current_df[['title', 'real']]\n",
    "current_df = current_df.rename(columns={'title':'statement', 'real':'truth'})\n",
    "current_df['truth'] = current_df['truth'].astype(int)\n",
    "current_df['statement'] = current_df['statement'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20e64cd2-fa99-43c5-90ac-543ed84d5923",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_df = current_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35324e93-5c7c-4caa-8c31-7007c56ce8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrased_statements = paraphrased_statements.apply(preprocess)\n",
    "paraphrased_df = pd.DataFrame({'statement': paraphrased_statements, 'truth': current_df['truth'][:len(current_df) // 2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64eb50a0-5c04-45f5-92c3-f3330c5b7bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_df = combine_dfs(paraphrased_df, current_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6bc4eed-7adc-437c-ae00-4751d5b86767",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = oversampler.fit_resample(current_df[['statement']], current_df['truth'])\n",
    "oversampled_df = pd.DataFrame({'statement': X_resampled['statement'], 'truth': y_resampled})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "522e4e82-a3b2-4bb2-b7fb-545ac7fa170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combine_dfs(df, oversampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3e8715d-15b4-4f4d-aa45-9be4af313a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_df = pd.read_csv('./KaggleFakeNews/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36242d6b-6576-459a-a2d0-b4a3d9ed7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_df = current_df.dropna(how='all')\n",
    "current_df = current_df.rename(columns={'text':'statement', 'label':'truth'})\n",
    "current_df = current_df[['statement', 'truth']]\n",
    "current_df['statement'] = current_df['statement'].astype(str).apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b8bd996-2547-4461-82d5-d7d28df89b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_df = current_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c57be10-056d-4508-aabc-8b58abb45c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrased_statements = pd.Series([paraphrase(statement) for statement in current_df['statement']])\n",
    "paraphrased_df = pd.DataFrame({'statement': paraphrased_statements, 'truth': current_df['truth'][:len(current_df) // 2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77851073-5dbd-40cb-ab72-608298c7aae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_df = combine_dfs(paraphrased_df, current_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "957d734e-9f43-40ab-8e27-d07bdc29d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_df = current_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9848d4b-c9d6-4f51-9b7b-8ea194225251",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = oversampler.fit_resample(current_df[['statement']], current_df['truth'])\n",
    "oversampled_df = pd.DataFrame({'statement': X_resampled['statement'], 'truth': y_resampled})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90f47a65-0c4d-4ffa-a86a-544f44ae1251",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combine_dfs(df, oversampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d33f652d-daff-4218-bfa7-8239e04ffe09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "truth\n",
       "1.0    49900\n",
       "0.0    49900\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['truth'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a8ff251-d28b-4b2c-8719-0a4cad914507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import issparse, csr_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb823d56-2151-406b-9a03-0b7494caa10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.grow_tree(X, y, 0)\n",
    "\n",
    "    def grow_tree(self, X, y, depth):\n",
    "\n",
    "        if depth >= self.max_depth or X.shape[0] < self.min_samples_split:\n",
    "            return np.bincount(y).argmax()\n",
    "\n",
    "        best_feature, best_threshold = self.find_best_split(X, y)\n",
    "        if best_feature is None:\n",
    "            return np.bincount(y).argmax()\n",
    "        X_col = X[:, best_feature].toarray().flatten()    \n",
    "        left_indices = X_col <= best_threshold\n",
    "        right_indices = X_col > best_threshold\n",
    "\n",
    "        if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
    "            # Return majority class if split is invalid\n",
    "            return np.bincount(y).argmax()\n",
    "        left = self.grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right = self.grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        return {'feature': best_feature, 'threshold': best_threshold, 'left': left, 'right': right}\n",
    "\n",
    "    def find_best_split(self, X, y):\n",
    "        #iterate through every split and test gini\n",
    "        n_features = X.shape[1]\n",
    "        features = np.random.choice(n_features, int(np.sqrt(n_features)), replace=False)\n",
    "        best_gini = 1.0\n",
    "        best_feature, best_threshold = None, None\n",
    "        for feature in features:\n",
    "            X_col = X[:, feature].toarray().flatten()\n",
    "            thresholds = np.unique(X_col[X_col > 0])\n",
    "            for threshold in thresholds:\n",
    "                left_indices = X_col <= threshold\n",
    "                right_indices = X_col > threshold\n",
    "                groups = [y[left_indices], y[right_indices]]\n",
    "\n",
    "                gini = gini_impurity(groups, np.unique(y))\n",
    "\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_threshold = threshold\n",
    "                    best_feature = feature\n",
    "            \n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _predict_tree(self, X):\n",
    "        predictions = []\n",
    "        for row in X:\n",
    "            node = self.tree\n",
    "            while isinstance(node, dict):\n",
    "                if row[node['feature']] <= node['threshold']:\n",
    "                    node = node['left']\n",
    "                else:\n",
    "                    node = node['right']\n",
    "            predictions.append(node)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f15256f-e3f1-4f13-b726-c22aeeb781b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(groups, classes):\n",
    "    n_instances = sum([len(group) for group in groups])\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = len(group)\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        for class_val in classes:\n",
    "            proportion = list(group).count(class_val) / size\n",
    "            score += proportion ** 2\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16723fe5-0734-4b25-96c7-dfa05f3c7b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=5, min_samples_split=2):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_trees):\n",
    "            X_sample, y_sample = random_sample(X, y)\n",
    "\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X, y)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_dense = X.toarray() if hasattr(X, \"toarray\") else X  # Handle sparse matrices\n",
    "        predictions = np.array([tree._predict_tree(X_dense) for tree in self.trees])\n",
    "        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2336937-32f2-4674-a052-80812c62f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(X, y):\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c47ef93a-2586-4c49-96cd-e03b1a54ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e1bbc6c-d9ad-4193-b47d-dff64abf3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0f2e0ef-967b-47e5-a9bd-67db5f135366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7aad927-de89-420f-bd8b-66a60d092a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "#print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96741faf-63a2-4339-8f3b-3e5e8b3083a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assume X parameter will be sparse\n",
    "#print(X_train[0].indices)\n",
    "#print(X_train[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d039ecad-0432-4f94-bdbd-f78ed8147186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf = RandomForest(n_trees=100, max_depth=10000)\n",
    "#rf.fit(X_train, y_train)\n",
    "\n",
    "#y_pred = rf.predict(X_test)\n",
    "\n",
    "#accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "#print(f'accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de989381-83cd-490e-bcd6-d94c021179ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb0e8a43-e519-4ab3-b1b9-20e18363aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MobileBertTokenizer, TFAutoModelForSequenceClassification, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8084530b-fbc6-4e9a-b0f6-01b1f2bbebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b196fe5-75e3-4f89-b8f4-403f526ffea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert = df['statement'].astype(str)\n",
    "y_bert = np.array(df['truth'])\n",
    "X_bert_train, X_bert_test, y_bert_train, y_bert_test = train_test_split(X_bert, y_bert, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "300b450d-8f0d-45bb-88a6-50b3840c1d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2c78d66-3446-45b0-83f9-bb86a26ff8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert_train = X_bert_train.tolist()\n",
    "X_bert_test = X_bert_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30591cc1-1b0b-4ba1-bc61-aaa5415d3c9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(\n",
    "    X_bert_train,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=64,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "test_tokenized = tokenizer(\n",
    "    X_bert_test,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=64,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38b2310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids = train_tokenized['input_ids']\n",
    "train_attention_masks = train_tokenized['attention_mask']\n",
    "train_labels = torch.tensor(y_bert_train, dtype=torch.long)\n",
    "\n",
    "test_input_ids = test_tokenized['input_ids']\n",
    "test_attention_mask = test_tokenized['attention_mask']\n",
    "test_labels = torch.tensor(y_bert_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f339c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ff8edad-f5c4-43a4-8501-12469a532f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "train_dl = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=16)\n",
    "class MyBertDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        '''\n",
    "        encodings: a dict of Tensors (e.g., 'input_ids', 'attention_mask') from tokenizer(..., return_tensors='pt')\n",
    "        labels: a list or tensor of labels (e.g., y_bert_train)\n",
    "        '''\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of examples\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For the given idx, gather all the tokenized inputs\n",
    "        input_ids = self.encodings['input_ids'][idx].clone().detach()\n",
    "        attention_mask = self.encodings['attention_mask'][idx].clone().detach()\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return input_ids, attention_mask, label\n",
    "    def __testgetitem__(self, idx):\n",
    "        # Suppose we load data from somewhere and convert it to tensors\n",
    "        input_ids = torch.tensor([...])\n",
    "        attention_mask = torch.tensor([...])\n",
    "        label = torch.tensor(...)\n",
    "        return input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "25323d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kazuj\\AppData\\Local\\Temp\\ipykernel_35648\\3737211109.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_input_ids = torch.tensor(train_input_ids, dtype=torch.long)\n",
      "C:\\Users\\kazuj\\AppData\\Local\\Temp\\ipykernel_35648\\3737211109.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_attention_masks = torch.tensor(train_attention_masks, dtype=torch.long)\n",
      "C:\\Users\\kazuj\\AppData\\Local\\Temp\\ipykernel_35648\\3737211109.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_labels = torch.tensor(train_labels, dtype=torch.long)  # or torch.float for regression\n",
      "C:\\Users\\kazuj\\AppData\\Local\\Temp\\ipykernel_35648\\3737211109.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_input_ids = torch.tensor(test_input_ids, dtype=torch.long)\n",
      "C:\\Users\\kazuj\\AppData\\Local\\Temp\\ipykernel_35648\\3737211109.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_attention_mask = torch.tensor(test_attention_mask, dtype=torch.long)\n",
      "C:\\Users\\kazuj\\AppData\\Local\\Temp\\ipykernel_35648\\3737211109.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_labels = torch.tensor(test_labels, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "train_input_ids = torch.tensor(train_input_ids, dtype=torch.long)\n",
    "train_attention_masks = torch.tensor(train_attention_masks, dtype=torch.long)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)  # or torch.float for regression\n",
    "\n",
    "test_input_ids = torch.tensor(test_input_ids, dtype=torch.long)\n",
    "test_attention_mask = torch.tensor(test_attention_mask, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fb9e221d-5d34-4345-8215-e1bded327171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyMobileBert were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MobileBertForSequenceClassification\n",
    "\n",
    "class MyMobileBert(MobileBertForSequenceClassification):\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, return_loss=False, **kwargs):\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            **kwargs\n",
    "        )\n",
    "        if return_loss and labels is not None:\n",
    "            return outputs.logits, outputs.loss\n",
    "        # Return only the logits, which is what the fastai Learner will treat as predictions\n",
    "        return outputs.logits\n",
    "\n",
    "\n",
    "# Then instantiate:\n",
    "model = MyMobileBert.from_pretrained(\"google/mobilebert-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb3a2189-3275-42c4-bd93-ce60a8c469f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MyBertDataset(train_tokenized, y_bert_train)\n",
    "test_ds  = MyBertDataset(test_tokenized,  y_bert_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d55ba10-4e8a-4e3a-b6af-a30b61a2751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    # Suppose each batch item is a tuple: (input_ids_list, attention_mask_list, label)\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "    \n",
    "    for b in batch:\n",
    "        input_ids.append(b[0])\n",
    "        attention_masks.append(b[1])\n",
    "        labels.append(b[2])\n",
    "        \n",
    "    # Convert them all to tensors\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return (input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83c15249-caf5-4dc3-84e3-5efe7465d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=2, collate_fn=collate)\n",
    "test_dl = DataLoader(test_ds, batch_size=2, collate_fn=collate)\n",
    "\n",
    "dls = DataLoaders(train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e5d190b-d929-4dd0-a3cb-c3c685797d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = .0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "876287c7-f9b0-48bd-ac49-9aa9851826ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kazuj\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "438e6b53-7040-4e00-bf47-6fdb2f0666dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "af0547b4-d398-49cf-8c4f-0a5db85b3612",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Average Validation Loss: 0.5592\n",
      "  Validation Accuracy: 69.02%\n",
      "Epoch 2:\n",
      "  Average Validation Loss: 0.4729\n",
      "  Validation Accuracy: 74.28%\n",
      "Epoch 3:\n",
      "  Average Validation Loss: 0.4190\n",
      "  Validation Accuracy: 77.19%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    for batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, return_loss=True)\n",
    "        loss = outputs[1]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dl:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            \n",
    "            # Forward pass with validation loss computation\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, return_loss=True)\n",
    "            logits = outputs[0]  # Logits from the model\n",
    "            val_loss = F.cross_entropy(logits, labels)  # Compute loss for this batch\n",
    "            \n",
    "            # Accumulate total loss\n",
    "            total_val_loss += val_loss.item()\n",
    "            \n",
    "            # Calculate predictions and accuracy\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = total_val_loss / len(test_dl)  # Average loss per batch\n",
    "    accuracy = correct / total  # Overall accuracy\n",
    "\n",
    "    # Print metrics for this epoch\n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    print(f\"  Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3c7045c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss\n",
    "}\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1a44b9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyMobileBert were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\kazuj\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('checkpoint.pth', weights_only=True)\n",
    "\n",
    "model = MyMobileBert.from_pretrained(\"google/mobilebert-uncased\", num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.train()\n",
    "\n",
    "start_epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e797bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "98df6646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "  Average Validation Loss: 0.4283\n",
      "  Validation Accuracy: 79.24%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_val_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dl:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        \n",
    "        # Forward pass with validation loss computation\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, return_loss=True)\n",
    "        logits = outputs[0]  # Logits from the model\n",
    "        val_loss = F.cross_entropy(logits, labels)  # Compute loss for this batch\n",
    "        \n",
    "        # Accumulate total loss\n",
    "        total_val_loss += val_loss.item()\n",
    "        \n",
    "        # Calculate predictions and accuracy\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "# Calculate average validation loss and accuracy\n",
    "avg_val_loss = total_val_loss / len(test_dl)  # Average loss per batch\n",
    "accuracy = correct / total  # Overall accuracy\n",
    "\n",
    "# Print metrics for this epoch\n",
    "print(f\"Epoch {epoch + 1}:\")\n",
    "print(f\"  Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "print(f\"  Validation Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7c2fcb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8ac66f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = 1e-5  # Set a reasonable learning rate, e.g., 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f8836ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Average Training Loss: 0.3369\n",
      "Epoch 4: Average Training Loss: 0.3240\n",
      "Epoch 5: Average Training Loss: 0.3122\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, 5):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move data to device\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs  # Raw logits from the model\n",
    "\n",
    "        # Compute loss manually\n",
    "        loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Print average training loss\n",
    "    avg_train_loss = total_train_loss / len(train_dl)\n",
    "    print(f\"Epoch {epoch + 1}: Average Training Loss: {avg_train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d8f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "  Average Validation Loss: 110.2467\n",
      "  Validation Accuracy: 49.91%\n",
      "Epoch 4:\n",
      "  Average Validation Loss: nan\n",
      "  Validation Accuracy: 49.91%\n",
      "Epoch 5:\n",
      "  Average Validation Loss: nan\n",
      "  Validation Accuracy: 49.91%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, 5):\n",
    "    model.train()\n",
    "    for batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, return_loss=True)\n",
    "        loss = outputs[1]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dl:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            \n",
    "            # Forward pass with validation loss computation\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, return_loss=True)\n",
    "            logits = outputs[0]  # Logits from the model\n",
    "            val_loss = F.cross_entropy(logits, labels)  # Compute loss for this batch\n",
    "            \n",
    "            # Accumulate total loss\n",
    "            total_val_loss += val_loss.item()\n",
    "            \n",
    "            # Calculate predictions and accuracy\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = total_val_loss / len(test_dl)  # Average loss per batch\n",
    "    accuracy = correct / total  # Overall accuracy\n",
    "\n",
    "    # Print metrics for this epoch\n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    print(f\"  Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "96c36247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyMobileBert were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens input ids torch.Size([1, 64])\n",
      "tokens attention mask torch.Size([1, 64])\n",
      "Prediction: True\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('checkpoint.pth', weights_only=True)\n",
    "\n",
    "model = MyMobileBert.from_pretrained(\"google/mobilebert-uncased\", num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')\n",
    "\n",
    "# Example headline\n",
    "headline = \"US Census found that 6 out of 10 americans are women\"\n",
    "\n",
    "# Preprocess the headline\n",
    "tokens = tokenizer(\n",
    "    headline,\n",
    "    max_length=64,  # Ensure this matches the max length used during training\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Move to the same device as the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "tokens = {key: val.to(device) for key, val in tokens.items()}\n",
    "\n",
    "print('tokens input ids', tokens['input_ids'].shape)\n",
    "print('tokens attention mask', tokens['attention_mask'].shape)\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    input_ids = tokens['input_ids']\n",
    "    attention_mask = tokens['attention_mask']\n",
    "\n",
    "\n",
    "    # Forward pass with validation loss computation\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, return_loss=True)\n",
    "    logits = outputs\n",
    "    prediction = torch.argmax(logits, dim=1).item()  # Compute loss for this batch  # Get the predicted label\n",
    "\n",
    "# Map prediction to label\n",
    "labels = {0: \"False\", 1: \"True\"}  # Update this mapping based on your dataset\n",
    "print(f\"Prediction: {labels[prediction]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd99d9-7ef8-40ea-9716-0a07474e7b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e8672-e902-4df3-8335-37d2eef6ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch from your train_dataloader\n",
    "\"\"\"batch = next(iter(train_dl))\n",
    "\n",
    "# If your batch is something like (input_ids, attention_mask, labels):\n",
    "input_ids, attention_mask, labels = batch\n",
    "\n",
    "# Now check their types:\n",
    "print(type(input_ids), type(attention_mask), type(labels))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "31cc296c-1788-4dfb-9b61-971f784d99e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.optimizer import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa050c-0458-4499-9926-6606c15f368f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    loss_func=CrossEntropyLossFlat(),\n",
    "    metrics=accuracy,\n",
    ")\n",
    "\n",
    "# 9) Use lr_find()\n",
    "lr = learn.lr_find()\n",
    "print(\"lr_find results:\", lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccb62da-b96b-4ebc-8bfe-3c3c7baf60d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717b035-b561-441e-9612-9b6f2f6bb424",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"my_model_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ba3fa-23d5-49ab-ae70-c89ba942acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the same model architecture\n",
    "model = MyMobileBert.from_pretrained(\"google/mobilebert-uncased\", num_labels=2)  # same class & config used before\n",
    "\n",
    "# Load state dict\n",
    "model.load_state_dict(torch.load(\"my_model_weights.pt\"))\n",
    "model.eval()  # or model.train() if you're continuing training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e14d9a-73ef-41db-bc94-ecb1b636a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8fa12-41bf-4851-80b5-1a6582f85817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import Policy, set_global_policy\n",
    "\n",
    "# Set the global policy for mixed precision\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)\n",
    "\n",
    "print(\"Mixed precision policy set:\", policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33d2a8-2c5d-455d-adde-e6fcdb0ccb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=3, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c221d2-f293-4b7c-8abd-0174a9af31ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./new_model')\n",
    "tokenizer.save_pretrained('./new_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc4a71-53a3-4e3e-99b2-dd9f966045f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"./Liar dataset/test.tsv\", sep=\"\\t\", names=column_names)\n",
    "\n",
    "label_map = {\n",
    "    'POF': 0,\n",
    "    'false': 0,\n",
    "    'barely-true': 0,\n",
    "    'half-true': 1,\n",
    "    'true': 1\n",
    "}\n",
    "label_columns = ['POF', 'false', 'barely-true', 'half-true', 'true']\n",
    "\n",
    "test_data = test_data.dropna(subset=label_columns, how='all')\n",
    "test_data = test_data[['statement', 'POF', 'false', 'barely-true', 'half-true', 'true']]\n",
    "test_data['label'] = test_data[label_columns].idxmax(axis=1)\n",
    "test_data['truth'] = test_data['label'].map(label_map)\n",
    "\n",
    "\"\"\"weighted_sum = sum([current_data[col] * label_map[col] for col in label_columns])\n",
    "total_counts = current_data[label_columns].sum(axis=1)\n",
    "current_data['confidence'] = round((weighted_sum / total_counts), 2)\"\"\"\n",
    "test_data = test_data[['statement', 'truth']]\n",
    "test_data['statement'] = test_data['statement'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa728d16-a131-4aee-8da2-138fb106d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = test_data['statement'].astype(str).tolist()\n",
    "labels = test_data['truth']\n",
    "\n",
    "tst_tokenized = tokenizer(\n",
    "    feats,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=64,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "tst_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': tst_tokenized['input_ids'], 'attention_mask': tst_tokenized['attention_mask']},\n",
    "    labels\n",
    ")).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b29b73-7802-4b8b-8b1a-dac386b20186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48ec6b-92c6-4450-95dc-75621f3db5e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = len(test_tokenized['input_ids']) // 32  # Adjust based on your hardware\n",
    "outputs = []\n",
    "\n",
    "for i in tqdm(range(0, len(test_tokenized['input_ids']), batch_size)):\n",
    "    batch_input_ids = test_tokenized['input_ids'][i:i+batch_size]\n",
    "    batch_attention_mask = test_tokenized['attention_mask'][i:i+batch_size]\n",
    "    batch_output = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "    outputs.append(batch_output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a64b643-b376-4898-acf5-cb0925bcedcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits = tf.concat(outputs, axis=0)\n",
    "\n",
    "print(all_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a88ffee-3026-428d-a5d4-62b914a8f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = tf.argmax(all_logits, axis=-1)\n",
    "correct_predictions = tf.reduce_sum(tf.cast(predicted_classes == y_bert_test, tf.float32))\n",
    "accuracy = correct_predictions / len(y_bert_test)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy.numpy():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f0674-98ff-4a7b-8ac3-d908757e6781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051dc16-0a4c-43f9-b022-558f3e8f0a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"current_data_1 = pd.read_csv('./dataset 2/dataset/gossipcop_fake.csv')\n",
    "current_data_1 = current_data_1.dropna(how='all')\n",
    "current_data_1 = current_data_1[['title']]\n",
    "current_data_1 = current_data_1.rename(columns={'title':'statement'})\n",
    "current_data_1['truth'] = 0\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436e46dc-0598-48af-94ce-494583d44686",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"current_data_2 = pd.read_csv('./dataset 2/dataset/gossipcop_real.csv')\n",
    "current_data_2 = current_data_2.dropna(how='all')\n",
    "current_data_2 = current_data_2[['title']]\n",
    "current_data_2 = current_data_2.rename(columns={'title':'statement'})\n",
    "current_data_2['truth'] = 1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376a01dd-a081-41ce-af1e-e75401608cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"current_data = pd.concat([current_data_1, current_data_2])\n",
    "X = current_data['statement']\n",
    "y = current_data['truth']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_vectorized, y)\n",
    "\n",
    "resampled_df = pd.DataFrame({'statement': vectorizer.inverse_transform(X_resampled),  # Attempt to reverse transform\n",
    "                             'truth': y_resampled})\n",
    "current_data = resampled_df\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea18fd-2697-41fa-89cb-b8db7578311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.concat([data, current_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ae85d-302c-470e-9675-fdcab90518df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"current_df_1 = pd.read_csv('./dataset 2/dataset/politifact_fake.csv')\n",
    "current_df_1 = current_df_1.dropna(how='all')\n",
    "current_df_1 = current_df_1[['title']]\n",
    "current_df_1 = current_df_1.rename(columns={'title':'statement'})\n",
    "current_df_1['truth'] = 0\n",
    "current_df_1['statement'] = current_df_1['statement'].apply(preprocess)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d45db9-1e16-45fe-a41e-0660969e2fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"current_df_2 = pd.read_csv('./dataset 2/dataset/politifact_real.csv')\n",
    "current_df_2 = current_df_2.dropna(how='all')\n",
    "current_df_2 = current_df_2[['title']]\n",
    "current_df_2 = current_df_2.rename(columns={'title':'statement'})\n",
    "current_df_2['truth'] = 1\n",
    "current_df_2['statement'] = current_df_2['statement'].apply(preprocess)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cd9a58-2328-4b8b-9d7f-4870b358a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_df = pd.concat([current_df_1, current_df_2])\n",
    "\"\"\"X = current_data['statement']\n",
    "y = current_data['truth']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_vectorized, y)\n",
    "\n",
    "resampled_df = pd.DataFrame({'statement': vectorizer.inverse_transform(X_resampled),  # Attempt to reverse transform\n",
    "                             'truth': y_resampled})\n",
    "current_data = resampled_df\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23019fe9-2a80-4960-b95c-80722f338503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8277c1-2200-4031-b7d4-96d6d587866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c81c0-d3a4-4cfa-9437-57e7571c14e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        ...\n",
    "        \n",
    "    def sigmoid(self, n):\n",
    "        return 1 / (1 + np.exp(-n))\n",
    "        \n",
    "    def initialize_weights(self, n_features):\n",
    "        weights = np.zeros(n_features)\n",
    "        bias = 0\n",
    "        return weights, bias\n",
    "        \n",
    "    def predict(self, X, weights, bias):\n",
    "        linear_model = X.dot(weights) + bias\n",
    "        predictions = self.sigmoid(linear_model)\n",
    "        return predictions\n",
    "        \n",
    "    def calculate_loss(self, y_true, y_pred):\n",
    "        n = len(y_true)\n",
    "        loss = (-1/n) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def gradient_descent(self, X, y, weights, bias, lr):\n",
    "        n = X.shape[0]\n",
    "    \n",
    "        y_pred = self.predict(X, weights, bias)\n",
    "    \n",
    "        dw = X.T.dot(y_pred - y) / n\n",
    "        db = np.sum(y_pred - y) / n\n",
    "    \n",
    "        weights -= lr * dw\n",
    "        bias -= lr * db\n",
    "    \n",
    "        return weights, bias\n",
    "\n",
    "    def train(self, X, y, lr=.1, epochs=1000, batch_size=500):\n",
    "        n_features = X.shape[1]\n",
    "    \n",
    "        weights, bias = self.initialize_weights(n_features)\n",
    "    \n",
    "        losses = []\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i:i + batch_size]\n",
    "                y_batch = y[i:i + batch_size]\n",
    "                \n",
    "                \n",
    "                weights, bias = self.gradient_descent(X_batch, y_batch, weights, bias, lr)\n",
    "    \n",
    "            y_pred = self.predict(X, weights, bias)\n",
    "            loss = self.calculate_loss(y, y_pred)\n",
    "            losses.append(loss)\n",
    "    \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "        return weights, bias, losses\n",
    "\n",
    "    def classify(self, X, weights, bias, threshold=.5):\n",
    "        probabilities = self.predict(X, weights, bias)\n",
    "        return [1 if p >= threshold else 0 for p in probabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5354f503-a2aa-408d-8349-cabd6a0bf291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE FOR THE SKLEARN MODEL\n",
    "#vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "#X = data['statement'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a4bd9f-4d6c-4346-9374-bf2c55864d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE FOR THE SKLEARN MODEL\n",
    "\"\"\"X_tfidf = vectorizer.fit_transform(X)\n",
    "y = np.array(data['truth'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3e855c-bd4c-4926-b812-61dd9b8f2ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=.2, random_state=42) #USE FOR THE SKLEARN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ad222-a6d1-4f50-8a0a-f212a8581cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DON'T USE FOR SKLEARN\n",
    "#regressor = LogisticRegression()\n",
    "#weights, bias, losses = regressor.train(X_train, y_train)\n",
    "#y_pred = regressor.classify(X_test, weights, bias)\n",
    "#print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "#print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c133e1b9-c4c2-487f-8aa4-deac4b638013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fbf28d-6a33-40ca-adc8-c08946200649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b0fe6-ecce-4dd0-b626-fb6f8d896be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "#model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc172d2-9f68-40c8-89b6-7cd79b849fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = model.predict(X_test)\n",
    "#y_prob = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b0741-90a3-477b-88fe-9ea8942a9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "#print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd98408-7393-4365-8b96-8063e9be447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fcae8e-ae90-406c-bbdf-33902626fc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
