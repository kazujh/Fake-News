{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14873286-6388-411a-bf9b-6422537739b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f0a0a7f-20d2-4ed7-8ff0-dff17a333994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc220ec1-0ef8-4c1f-aa26-9be56d1da4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6925d511-4681-4e6b-9124-60baefa0aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "701ef617-f34d-4486-878a-db91eb1354e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(file_directory):\n",
    "    if os.path.isdir(os.path.join(file_directory, f)):\n",
    "        folder = os.path.join(file_directory, f)\n",
    "        for d in os.listdir(folder):\n",
    "            if d[-2::] == 'sv':\n",
    "                data = pd.read_csv(os.path.join(folder, d),sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffbc0b20-aede-4eaa-9546-5f458588b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['id', 'truth-value', 'statement', 'topics', 'speaker', 'speaker occupation', 'state', 'party', 'barely-true', 'false', 'half-true', 'true', 'POF', 'context']\n",
    "data = pd.DataFrame()\n",
    "current_data = pd.read_csv(\"./Liar dataset/train.tsv\", sep=\"\\t\", names=column_names)\n",
    "\n",
    "label_map = {\n",
    "    'POF': 0,\n",
    "    'false': 0,\n",
    "    'barely-true': 0,\n",
    "    'half-true': 1,\n",
    "    'true': 1\n",
    "}\n",
    "label_columns = ['POF', 'false', 'barely-true', 'half-true', 'true']\n",
    "\n",
    "current_data = current_data.dropna(subset=label_columns, how='all')\n",
    "current_data = current_data[['statement', 'POF', 'false', 'barely-true', 'half-true', 'true']]\n",
    "current_data['label'] = current_data[label_columns].idxmax(axis=1)\n",
    "current_data['truth'] = current_data['label'].map(label_map)\n",
    "\n",
    "\"\"\"weighted_sum = sum([current_data[col] * label_map[col] for col in label_columns])\n",
    "total_counts = current_data[label_columns].sum(axis=1)\n",
    "current_data['confidence'] = round((weighted_sum / total_counts), 2)\"\"\"\n",
    "current_data = current_data[['statement', 'truth']]\n",
    "current_data['statement'] = current_data['statement'].apply(preprocess)\n",
    "data = pd.concat([data, current_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb21cec9-6e34-41f0-9fa6-9c41f8338932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_data = pd.read_csv('./dataset 1/FakeNewsNet.csv')\n",
    "\n",
    "current_data = current_data[['title', 'real']]\n",
    "current_data = current_data.rename(columns={'title':'statement', 'real':'truth'})\n",
    "current_data['truth'] = current_data['truth'].astype(int)\n",
    "current_data['statement'] = current_data['statement'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a10f81fe-3507-4e25-ab3f-191901cbbd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6bc4eed-7adc-437c-ae00-4751d5b86767",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = current_data['statement']\n",
    "y = current_data['truth']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_vectorized, y)\n",
    "\n",
    "resampled_df = pd.DataFrame({'statement': vectorizer.inverse_transform(X_resampled),  # Attempt to reverse transform\n",
    "                             'truth': y_resampled})\n",
    "current_data = resampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "522e4e82-a3b2-4bb2-b7fb-545ac7fa170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, current_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d051dc16-0a4c-43f9-b022-558f3e8f0a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data_1 = pd.read_csv('./dataset 2/dataset/gossipcop_fake.csv')\n",
    "current_data_1 = current_data_1.dropna(how='all')\n",
    "current_data_1 = current_data_1[['title']]\n",
    "current_data_1 = current_data_1.rename(columns={'title':'statement'})\n",
    "current_data_1['truth'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "436e46dc-0598-48af-94ce-494583d44686",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data_2 = pd.read_csv('./dataset 2/dataset/gossipcop_real.csv')\n",
    "current_data_2 = current_data_2.dropna(how='all')\n",
    "current_data_2 = current_data_2[['title']]\n",
    "current_data_2 = current_data_2.rename(columns={'title':'statement'})\n",
    "current_data_2['truth'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "376a01dd-a081-41ce-af1e-e75401608cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data = pd.concat([current_data_1, current_data_2])\n",
    "X = current_data['statement']\n",
    "y = current_data['truth']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_vectorized, y)\n",
    "\n",
    "resampled_df = pd.DataFrame({'statement': vectorizer.inverse_transform(X_resampled),  # Attempt to reverse transform\n",
    "                             'truth': y_resampled})\n",
    "current_data = resampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfea18fd-2697-41fa-89cb-b8db7578311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, current_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a59ae85d-302c-470e-9675-fdcab90518df",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data_1 = pd.read_csv('./dataset 2/dataset/politifact_fake.csv')\n",
    "current_data_1 = current_data_1.dropna(how='all')\n",
    "current_data_1 = current_data_1[['title']]\n",
    "current_data_1 = current_data_1.rename(columns={'title':'statement'})\n",
    "current_data_1['truth'] = 0\n",
    "current_data_1['statement'] = current_data_1['statement'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0d45db9-1e16-45fe-a41e-0660969e2fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data_2 = pd.read_csv('./dataset 2/dataset/politifact_real.csv')\n",
    "current_data_2 = current_data_2.dropna(how='all')\n",
    "current_data_2 = current_data_2[['title']]\n",
    "current_data_2 = current_data_2.rename(columns={'title':'statement'})\n",
    "current_data_2['truth'] = 1\n",
    "current_data_2['statement'] = current_data_2['statement'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60cd9a58-2328-4b8b-9d7f-4870b358a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data = pd.concat([current_data_1, current_data_2])\n",
    "X = current_data['statement']\n",
    "y = current_data['truth']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_vectorized, y)\n",
    "\n",
    "resampled_df = pd.DataFrame({'statement': vectorizer.inverse_transform(X_resampled),  # Attempt to reverse transform\n",
    "                             'truth': y_resampled})\n",
    "current_data = resampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "366eb0da-c3f0-4659-a661-0dc80109f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, current_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23019fe9-2a80-4960-b95c-80722f338503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d8277c1-2200-4031-b7d4-96d6d587866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b98c81c0-d3a4-4cfa-9437-57e7571c14e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        ...\n",
    "        \n",
    "    def sigmoid(self, n):\n",
    "        return 1 / (1 + np.exp(-n))\n",
    "        \n",
    "    def initialize_weights(self, n_features):\n",
    "        weights = np.zeros(n_features)\n",
    "        bias = 0\n",
    "        return weights, bias\n",
    "        \n",
    "    def predict(self, X, weights, bias):\n",
    "        linear_model = X.dot(weights) + bias\n",
    "        predictions = self.sigmoid(linear_model)\n",
    "        return predictions\n",
    "        \n",
    "    def calculate_loss(self, y_true, y_pred):\n",
    "        n = len(y_true)\n",
    "        loss = (-1/n) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def gradient_descent(self, X, y, weights, bias, lr):\n",
    "        n = X.shape[0]\n",
    "    \n",
    "        y_pred = self.predict(X, weights, bias)\n",
    "    \n",
    "        dw = X.T.dot(y_pred - y) / n\n",
    "        db = np.sum(y_pred - y) / n\n",
    "    \n",
    "        weights -= lr * dw\n",
    "        bias -= lr * db\n",
    "    \n",
    "        return weights, bias\n",
    "\n",
    "    def train(self, X, y, lr=.1, epochs=1000, batch_size=500):\n",
    "        n_features = X.shape[1]\n",
    "    \n",
    "        weights, bias = self.initialize_weights(n_features)\n",
    "    \n",
    "        losses = []\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i:i + batch_size]\n",
    "                y_batch = y[i:i + batch_size]\n",
    "                \n",
    "                \n",
    "                weights, bias = self.gradient_descent(X_batch, y_batch, weights, bias, lr)\n",
    "    \n",
    "            y_pred = self.predict(X, weights, bias)\n",
    "            loss = self.calculate_loss(y, y_pred)\n",
    "            losses.append(loss)\n",
    "    \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "        return weights, bias, losses\n",
    "\n",
    "    def classify(self, X, weights, bias, threshold=.5):\n",
    "        probabilities = self.predict(X, weights, bias)\n",
    "        return [1 if p >= threshold else 0 for p in probabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5354f503-a2aa-408d-8349-cabd6a0bf291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE FOR THE SKLEARN MODEL\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "X = data['statement'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89a4bd9f-4d6c-4346-9374-bf2c55864d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE FOR THE SKLEARN MODEL\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "y = np.array(data['truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b3e855c-bd4c-4926-b812-61dd9b8f2ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=.2, random_state=42) #USE FOR THE SKLEARN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "721ad222-a6d1-4f50-8a0a-f212a8581cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DON'T USE FOR SKLEARN\n",
    "#regressor = LogisticRegression()\n",
    "#weights, bias, losses = regressor.train(X_train, y_train)\n",
    "#y_pred = regressor.classify(X_test, weights, bias)\n",
    "#print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "#print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c133e1b9-c4c2-487f-8aa4-deac4b638013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12fbf28d-6a33-40ca-adc8-c08946200649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e2b0fe6-ecce-4dd0-b626-fb6f8d896be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "#model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cc172d2-9f68-40c8-89b6-7cd79b849fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = model.predict(X_test)\n",
    "#y_prob = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c2b0741-90a3-477b-88fe-9ea8942a9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "#print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cd98408-7393-4365-8b96-8063e9be447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a8ff251-d28b-4b2c-8719-0a4cad914507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import issparse, csr_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb823d56-2151-406b-9a03-0b7494caa10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.grow_tree(X, y, 0)\n",
    "\n",
    "    def grow_tree(self, X, y, depth):\n",
    "\n",
    "        if depth >= self.max_depth or X.shape[0] < self.min_samples_split:\n",
    "            return np.bincount(y).argmax()\n",
    "\n",
    "        best_feature, best_threshold = self.find_best_split(X, y)\n",
    "        if best_feature is None:\n",
    "            return np.bincount(y).argmax()\n",
    "        X_col = X[:, best_feature].toarray().flatten()    \n",
    "        left_indices = X_col <= best_threshold\n",
    "        right_indices = X_col > best_threshold\n",
    "\n",
    "        if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
    "            # Return majority class if split is invalid\n",
    "            return np.bincount(y).argmax()\n",
    "        left = self.grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right = self.grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        return {'feature': best_feature, 'threshold': best_threshold, 'left': left, 'right': right}\n",
    "\n",
    "    def find_best_split(self, X, y):\n",
    "        #iterate through every split and test gini\n",
    "        n_features = X.shape[1]\n",
    "        features = np.random.choice(n_features, int(np.sqrt(n_features)), replace=False)\n",
    "        best_gini = 1.0\n",
    "        best_feature, best_threshold = None, None\n",
    "        for feature in features:\n",
    "            X_col = X[:, feature].toarray().flatten()\n",
    "            thresholds = np.unique(X_col[X_col > 0])\n",
    "            for threshold in thresholds:\n",
    "                left_indices = X_col <= threshold\n",
    "                right_indices = X_col > threshold\n",
    "                groups = [y[left_indices], y[right_indices]]\n",
    "\n",
    "                gini = gini_impurity(groups, np.unique(y))\n",
    "\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_threshold = threshold\n",
    "                    best_feature = feature\n",
    "            \n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _predict_tree(self, X):\n",
    "        predictions = []\n",
    "        for row in X:\n",
    "            node = self.tree\n",
    "            while isinstance(node, dict):\n",
    "                if row[node['feature']] <= node['threshold']:\n",
    "                    node = node['left']\n",
    "                else:\n",
    "                    node = node['right']\n",
    "            predictions.append(node)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f15256f-e3f1-4f13-b726-c22aeeb781b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(groups, classes):\n",
    "    n_instances = sum([len(group) for group in groups])\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = len(group)\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        for class_val in classes:\n",
    "            proportion = list(group).count(class_val) / size\n",
    "            score += proportion ** 2\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16723fe5-0734-4b25-96c7-dfa05f3c7b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=5, min_samples_split=2):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_trees):\n",
    "            X_sample, y_sample = random_sample(X, y)\n",
    "\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X, y)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_dense = X.toarray() if hasattr(X, \"toarray\") else X  # Handle sparse matrices\n",
    "        predictions = np.array([tree._predict_tree(X_dense) for tree in self.trees])\n",
    "        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2336937-32f2-4674-a052-80812c62f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(X, y):\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c47ef93a-2586-4c49-96cd-e03b1a54ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e1bbc6c-d9ad-4193-b47d-dff64abf3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0f2e0ef-967b-47e5-a9bd-67db5f135366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7aad927-de89-420f-bd8b-66a60d092a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "#print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96741faf-63a2-4339-8f3b-3e5e8b3083a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assume X parameter will be sparse\n",
    "#print(X_train[0].indices)\n",
    "#print(X_train[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d039ecad-0432-4f94-bdbd-f78ed8147186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf = RandomForest(n_trees=100, max_depth=10000)\n",
    "#rf.fit(X_train, y_train)\n",
    "\n",
    "#y_pred = rf.predict(X_test)\n",
    "\n",
    "#accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "#print(f'accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb0e8a43-e519-4ab3-b1b9-20e18363aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import MobileBertTokenizer, TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b196fe5-75e3-4f89-b8f4-403f526ffea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert = data['statement'].astype(str)\n",
    "y_bert = data['truth']\n",
    "X_bert_train, X_bert_test, y_bert_train, y_bert_test = train_test_split(X_bert, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "300b450d-8f0d-45bb-88a6-50b3840c1d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2c78d66-3446-45b0-83f9-bb86a26ff8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert_train = X_bert_train.tolist()\n",
    "X_bert_test = X_bert_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "30591cc1-1b0b-4ba1-bc61-aaa5415d3c9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(\n",
    "    X_bert_train,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=64,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "test_tokenized = tokenizer(\n",
    "    X_bert_test,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=64,\n",
    "    return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ff8edad-f5c4-43a4-8501-12469a532f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': train_tokenized['input_ids'], 'attention_mask': train_tokenized['attention_mask']},\n",
    "    y_bert_train\n",
    ")).shuffle(len(X_bert_train)).batch(16)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': test_tokenized['input_ids'], 'attention_mask': test_tokenized['attention_mask']},\n",
    "    y_bert_test\n",
    ")).batch(16)\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f98f8317-1073-48fb-8063-e5dd9fcb52f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMobileBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFMobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained('google/mobilebert-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "876287c7-f9b0-48bd-ac49-9aa9851826ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamWeightDecay\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamWeightDecay(learning_rate=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "16e14d9a-73ef-41db-bc94-ecb1b636a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "79f8fa12-41bf-4851-80b5-1a6582f85817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed precision policy set: <DTypePolicy \"mixed_float16\">\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.mixed_precision import Policy, set_global_policy\n",
    "\n",
    "# Set the global policy for mixed precision\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)\n",
    "\n",
    "print(\"Mixed precision policy set:\", policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33d2a8-2c5d-455d-adde-e6fcdb0ccb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "  49/4001 [..............................] - ETA: 1:41:58 - loss: 0.6538 - accuracy: 0.6250"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=3, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48ec6b-92c6-4450-95dc-75621f3db5e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = len(test_tokenized['input_ids']) // 32  # Adjust based on your hardware\n",
    "outputs = []\n",
    "\n",
    "for i in tqdm(range(0, len(test_tokenized['input_ids']), batch_size)):\n",
    "    batch_input_ids = test_tokenized['input_ids'][i:i+batch_size]\n",
    "    batch_attention_mask = test_tokenized['attention_mask'][i:i+batch_size]\n",
    "    batch_output = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "    outputs.append(batch_output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a64b643-b376-4898-acf5-cb0925bcedcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits = tf.concat(outputs, axis=0)\n",
    "\n",
    "print(all_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a88ffee-3026-428d-a5d4-62b914a8f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = tf.argmax(all_logits, axis=-1)\n",
    "correct_predictions = tf.reduce_sum(tf.cast(predicted_classes == y_bert_test, tf.float32))\n",
    "accuracy = correct_predictions / len(y_bert_test)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy.numpy():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "232f0674-98ff-4a7b-8ac3-d908757e6781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%nvidia-smi` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8403a18b-fdf7-4f63-910c-84dd4be64dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
